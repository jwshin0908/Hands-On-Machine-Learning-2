{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6ec00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬 ≥3.5 필수\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# 사이킷런 ≥0.20 필수\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# 공통 모듈 임포트\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 노트북 실행 결과를 동일하게 유지하기 위해\n",
    "np.random.seed(42)\n",
    "\n",
    "# 깔끔한 그래프 출력을 위해\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# 그림을 저장할 위치\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ensembles\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"그림 저장:\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0239fc51",
   "metadata": {},
   "source": [
    "**앙상블 학습(ensemble learning)** : 일련의 예측기로부터 예측을 수집하여 가장 좋은 모델 하나보다 더 좋은 예측을 얻는 방법\n",
    "+ 예측기가 가능한 한 서로 독립적일 때 최고의 성능을 발휘함(각각 다른 알고리즘으로 학습)\n",
    "\n",
    "**랜덤 포레스트(random forest)** : 모든 개별 트리의 예측을 구한 뒤 가장 많은 선택을 받은 클래스를 예측으로 삼는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90134118",
   "metadata": {},
   "source": [
    "## 7.1 투표 기반 분류기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25446162",
   "metadata": {},
   "source": [
    "**직접 투표(hard voting)** : 각 분류기의 예측을 모아서 가장 많이 선택된 클래스를 예측하는 것 (다수결로 정해지는 분류기)\n",
    "+ 각 분류기가 약한 학습기일지라도 충분하게 많고 다양하다면 앙상블은 강한 학습기가 될 수 있음\n",
    "\n",
    "<img src=\"img/7-2.png\" width=\"500px\" align='left'>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55aa11a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcc2c566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(random_state=42)),\n",
       "                             ('rf', RandomForestClassifier(random_state=42)),\n",
       "                             ('svc', SVC(random_state=42))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여러 분류기를 조합하여 사이킷런의 투표 기반 분류기(VotingClassifier)을 만들고 훈련시킴\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11d80156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611d2b0e",
   "metadata": {},
   "source": [
    "**간접 투표(soft voting)** : 모든 분류기가 클래스의 확률을 예측할 수 있으면 개별 분류기의 예측을 평균 내어 확률이 가장 높은 클래스를 예측할 수 있음\n",
    "+ 확률이 높은 투표에 비중을 더 두기 때문에 직접 투표 방식보다 성능이 높음\n",
    "+ voting = 'soft'로 사용\n",
    "+ SVC는 기본값에서 클래스 확률을 제공하지 않기에 probability 매개변수를 True로 지정(속도가 느려지지만 predict_proba() 메서드를 사용가능)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a1f2c",
   "metadata": {},
   "source": [
    "## 7.2 배깅과 페이스팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fee1ff3",
   "metadata": {},
   "source": [
    "**다양한 분류기를 만드는 방법**\n",
    "+ 각기 다른 훈련 알고리즘을 사용하는 것\n",
    "+ 같은 알고리즘을 사용하고 훈련 세트의 서브셋을 무작위로 구성하여 분류기를 각기 다르게 학습\n",
    "\n",
    "**배깅(bagging, bootstrap aggregating)** : 훈련 세트에서 중복을 허용하여 샘플링하는 방식\n",
    "+ 한 예측기를 위해 같은 훈련 샘플을 여러 번 샘플링 가능\n",
    "\n",
    "**페이스팅(pasting)** : 중복을 허용하지 않고 샘플링하는 방식\n",
    "\n",
    "**샘플링과 훈련 과정**\n",
    "+ 모든 예측기가 훈련을 마치면 앙상블은 모든 예측기의 예측을 모아서 새로운 샘플에 대한 예측을 만듦\n",
    "+ 수집 함수를 통과하면 편향과 분산이 감소 : 분류 - 통계적 최빈값 / 회귀 - 평균\n",
    "+ 앙상블의 결과는 하나의 예측기를 훈련시킬 때보다 편향은 비슷하지만 분산이 감소\n",
    "+ 예측기는 모두 동시에 다른 CPU 코어나 서버에서 병렬로 학습 가능(+ 예측도 병렬로 가능)\n",
    "\n",
    "<img src=\"img/7-4.png\" width=\"500px\" align='left'>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30030f36",
   "metadata": {},
   "source": [
    "### 7.2.1 사이킷런의 배깅과 페이스팅\n",
    "**BaggingClassifier (회귀의 경우 BaggingRegressor)**\n",
    "+ max_samples : 샘플링되는 데이터 수(0~1로 지정할 경우 훈련 세트의 크기에 곱한 값)\n",
    "+ bootstrap : True = 배깅, False = 페이스팅\n",
    "+ n_jobs : 사이킷런이 훈련과 예측에 사용할 CPU 코어 수(-1일 경우 모든 코어 사용)\n",
    "+ 기반 분류기가 결정 트리 분류기처럼 클래스 확률을 추정할 수 있으면 직접 투표 대신 간접 투표 방식을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e7300de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15638ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5cdf0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856\n"
     ]
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d94fe",
   "metadata": {},
   "source": [
    "앙상블의 예측이 결정 트리 하나의 예측보다 일반화가 훨씬 잘 됨(비슷한 편향에서 더 작은 분산)  \n",
    "배깅이 페이스팅보다 편향이 조금 더 높지만 분산은 감소 -> 전반적으로 배깅이 더 나은 모델\n",
    "\n",
    "<img src=\"img/7-5.png\" width=\"500px\" align='left'>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb072fe9",
   "metadata": {},
   "source": [
    "### 7.2.2 oob 평가\n",
    "BaggingClassifier은 기본값으로 중복을 허용하여(bootstrap = True) 훈련 세트의 크기만큼인 m개 샘플을 선택\n",
    "\n",
    "**oob(out-of-bag) 샘플**\n",
    "+ 각 예측기에 훈련 샘플의 63% 정도만 샘플링되고, 선택되지 않은 나머지 37% 샘플(예측기마다 남은 37%는 다름)\n",
    "+ 별도의 검증 세트를 사용하지 않고 oob 샘플을 사용해 평가\n",
    "+ 앙상블의 평가는 각 예측기의 oob 평가를 평균한 값\n",
    "+ oob_score = True 로 지정하면 자동으로 oob 평가를 수행\n",
    "+ oob_decision_function_ : 각 훈련 샘플의 클래스 확률을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70bd0f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8986666666666666"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oob_score=True\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    bootstrap=True, oob_score=True, random_state=40)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ee2a8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oob 평가와 비슷한 테스트 세트 정확도\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbec9a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32275132, 0.67724868],\n",
       "       [0.34117647, 0.65882353],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.09497207, 0.90502793],\n",
       "       [0.31147541, 0.68852459],\n",
       "       [0.01754386, 0.98245614],\n",
       "       [0.97109827, 0.02890173],\n",
       "       [0.97765363, 0.02234637],\n",
       "       [0.74404762, 0.25595238],\n",
       "       [0.        , 1.        ],\n",
       "       [0.7173913 , 0.2826087 ],\n",
       "       [0.85026738, 0.14973262],\n",
       "       [0.97222222, 0.02777778],\n",
       "       [0.0625    , 0.9375    ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97837838, 0.02162162],\n",
       "       [0.94642857, 0.05357143],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01704545, 0.98295455],\n",
       "       [0.39473684, 0.60526316],\n",
       "       [0.88700565, 0.11299435],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97790055, 0.02209945],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99428571, 0.00571429],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.62569832, 0.37430168],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.13402062, 0.86597938],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.38251366, 0.61748634],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.27093596, 0.72906404],\n",
       "       [0.34146341, 0.65853659],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00531915, 0.99468085],\n",
       "       [0.98843931, 0.01156069],\n",
       "       [0.91428571, 0.08571429],\n",
       "       [0.97282609, 0.02717391],\n",
       "       [0.98019802, 0.01980198],\n",
       "       [0.        , 1.        ],\n",
       "       [0.07361963, 0.92638037],\n",
       "       [0.98019802, 0.01980198],\n",
       "       [0.0052356 , 0.9947644 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97790055, 0.02209945],\n",
       "       [0.8       , 0.2       ],\n",
       "       [0.42424242, 0.57575758],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.66477273, 0.33522727],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.86781609, 0.13218391],\n",
       "       [1.        , 0.        ],\n",
       "       [0.56725146, 0.43274854],\n",
       "       [0.1576087 , 0.8423913 ],\n",
       "       [0.66492147, 0.33507853],\n",
       "       [0.91709845, 0.08290155],\n",
       "       [0.        , 1.        ],\n",
       "       [0.16759777, 0.83240223],\n",
       "       [0.87434555, 0.12565445],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.995     , 0.005     ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.07878788, 0.92121212],\n",
       "       [0.05418719, 0.94581281],\n",
       "       [0.29015544, 0.70984456],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.83040936, 0.16959064],\n",
       "       [0.01092896, 0.98907104],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.21465969, 0.78534031],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.94660194, 0.05339806],\n",
       "       [0.77094972, 0.22905028],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.16574586, 0.83425414],\n",
       "       [0.65306122, 0.34693878],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02564103, 0.97435897],\n",
       "       [0.50555556, 0.49444444],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03208556, 0.96791444],\n",
       "       [0.99435028, 0.00564972],\n",
       "       [0.23699422, 0.76300578],\n",
       "       [0.49509804, 0.50490196],\n",
       "       [0.9947644 , 0.0052356 ],\n",
       "       [0.00555556, 0.99444444],\n",
       "       [0.98963731, 0.01036269],\n",
       "       [0.26153846, 0.73846154],\n",
       "       [0.92972973, 0.07027027],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.80113636, 0.19886364],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0106383 , 0.9893617 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98181818, 0.01818182],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01036269, 0.98963731],\n",
       "       [0.97752809, 0.02247191],\n",
       "       [0.99453552, 0.00546448],\n",
       "       [0.01960784, 0.98039216],\n",
       "       [0.17857143, 0.82142857],\n",
       "       [0.98387097, 0.01612903],\n",
       "       [0.29533679, 0.70466321],\n",
       "       [0.98295455, 0.01704545],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00561798, 0.99438202],\n",
       "       [0.75690608, 0.24309392],\n",
       "       [0.38624339, 0.61375661],\n",
       "       [0.40625   , 0.59375   ],\n",
       "       [0.87368421, 0.12631579],\n",
       "       [0.92462312, 0.07537688],\n",
       "       [0.05181347, 0.94818653],\n",
       "       [0.82802548, 0.17197452],\n",
       "       [0.01546392, 0.98453608],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02298851, 0.97701149],\n",
       "       [0.9726776 , 0.0273224 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01041667, 0.98958333],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03804348, 0.96195652],\n",
       "       [0.02040816, 0.97959184],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94915254, 0.05084746],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99462366, 0.00537634],\n",
       "       [0.        , 1.        ],\n",
       "       [0.39378238, 0.60621762],\n",
       "       [0.33152174, 0.66847826],\n",
       "       [0.00609756, 0.99390244],\n",
       "       [0.        , 1.        ],\n",
       "       [0.3172043 , 0.6827957 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00588235, 0.99411765],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98924731, 0.01075269],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.62893082, 0.37106918],\n",
       "       [0.92344498, 0.07655502],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99526066, 0.00473934],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98888889, 0.01111111],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.06989247, 0.93010753],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03608247, 0.96391753],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02185792, 0.97814208],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95808383, 0.04191617],\n",
       "       [0.78362573, 0.21637427],\n",
       "       [0.56650246, 0.43349754],\n",
       "       [0.        , 1.        ],\n",
       "       [0.18023256, 0.81976744],\n",
       "       [1.        , 0.        ],\n",
       "       [0.93121693, 0.06878307],\n",
       "       [0.97175141, 0.02824859],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00531915, 0.99468085],\n",
       "       [0.        , 1.        ],\n",
       "       [0.43010753, 0.56989247],\n",
       "       [0.85858586, 0.14141414],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00558659, 0.99441341],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96923077, 0.03076923],\n",
       "       [0.        , 1.        ],\n",
       "       [0.21649485, 0.78350515],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98477157, 0.01522843],\n",
       "       [0.8       , 0.2       ],\n",
       "       [0.99441341, 0.00558659],\n",
       "       [0.        , 1.        ],\n",
       "       [0.09497207, 0.90502793],\n",
       "       [0.99492386, 0.00507614],\n",
       "       [0.01714286, 0.98285714],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02747253, 0.97252747],\n",
       "       [1.        , 0.        ],\n",
       "       [0.77005348, 0.22994652],\n",
       "       [0.        , 1.        ],\n",
       "       [0.90229885, 0.09770115],\n",
       "       [0.98387097, 0.01612903],\n",
       "       [0.22222222, 0.77777778],\n",
       "       [0.20348837, 0.79651163],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.20338983, 0.79661017],\n",
       "       [0.98181818, 0.01818182],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98969072, 0.01030928],\n",
       "       [0.        , 1.        ],\n",
       "       [0.48663102, 0.51336898],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00529101, 0.99470899],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.08379888, 0.91620112],\n",
       "       [0.12352941, 0.87647059],\n",
       "       [0.99415205, 0.00584795],\n",
       "       [0.03517588, 0.96482412],\n",
       "       [1.        , 0.        ],\n",
       "       [0.39790576, 0.60209424],\n",
       "       [0.05434783, 0.94565217],\n",
       "       [0.53191489, 0.46808511],\n",
       "       [0.51898734, 0.48101266],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.60869565, 0.39130435],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.24157303, 0.75842697],\n",
       "       [0.81578947, 0.18421053],\n",
       "       [0.08717949, 0.91282051],\n",
       "       [0.99453552, 0.00546448],\n",
       "       [0.82142857, 0.17857143],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.11904762, 0.88095238],\n",
       "       [0.04188482, 0.95811518],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.89150943, 0.10849057],\n",
       "       [0.19230769, 0.80769231],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.00515464, 0.99484536],\n",
       "       [0.59375   , 0.40625   ],\n",
       "       [0.07692308, 0.92307692],\n",
       "       [0.99484536, 0.00515464],\n",
       "       [0.83684211, 0.16315789],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99484536, 0.00515464],\n",
       "       [0.95360825, 0.04639175],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.26395939, 0.73604061],\n",
       "       [0.98461538, 0.01538462],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00574713, 0.99425287],\n",
       "       [0.85142857, 0.14857143],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.75301205, 0.24698795],\n",
       "       [0.8969697 , 0.1030303 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.75555556, 0.24444444],\n",
       "       [0.48863636, 0.51136364],\n",
       "       [0.        , 1.        ],\n",
       "       [0.92473118, 0.07526882],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.87709497, 0.12290503],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.74752475, 0.25247525],\n",
       "       [0.09146341, 0.90853659],\n",
       "       [0.42268041, 0.57731959],\n",
       "       [0.22395833, 0.77604167],\n",
       "       [0.        , 1.        ],\n",
       "       [0.87046632, 0.12953368],\n",
       "       [0.78212291, 0.21787709],\n",
       "       [0.00507614, 0.99492386],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02884615, 0.97115385],\n",
       "       [0.96      , 0.04      ],\n",
       "       [0.93478261, 0.06521739],\n",
       "       [1.        , 0.        ],\n",
       "       [0.50731707, 0.49268293],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01604278, 0.98395722],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96987952, 0.03012048],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05172414, 0.94827586],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99494949, 0.00505051],\n",
       "       [0.01675978, 0.98324022],\n",
       "       [1.        , 0.        ],\n",
       "       [0.14583333, 0.85416667],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [0.        , 1.        ],\n",
       "       [0.41836735, 0.58163265],\n",
       "       [0.13095238, 0.86904762],\n",
       "       [0.22110553, 0.77889447],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97647059, 0.02352941],\n",
       "       [0.21195652, 0.78804348],\n",
       "       [0.98882682, 0.01117318],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96428571, 0.03571429],\n",
       "       [0.34554974, 0.65445026],\n",
       "       [0.98235294, 0.01764706],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99465241, 0.00534759],\n",
       "       [0.        , 1.        ],\n",
       "       [0.06043956, 0.93956044],\n",
       "       [0.98214286, 0.01785714],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03108808, 0.96891192],\n",
       "       [0.58854167, 0.41145833]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1687fbd7",
   "metadata": {},
   "source": [
    "## 7.3 랜덤 패치와 랜덤 서브스페이스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da1e5f4",
   "metadata": {},
   "source": [
    "특성 샘플링 : max_features, bootstrap_features 두 매개변수로 조절\n",
    "+ 각 예측기는 무작위로 선택한 입력 특성의 일부분으로 훈련\n",
    "+ (이미지와 같은) 매우 차원의 데이터셋을 다룰 때 유용\n",
    "\n",
    "**랜덤 패치 방식(random patches method)** : 훈련 특성과 샘플을 모두 샘플링\n",
    "\n",
    "**랜덤 서브스페이스 방식(random subspaces method)** : 훈련 샘플을 모두 사용하고 특성은 샘플링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10e0329",
   "metadata": {},
   "source": [
    "## 7.4 랜덤 포레스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f341ef1d",
   "metadata": {},
   "source": [
    "**랜덤 포레스트(random forest)**\n",
    "+ 일반적으로 배깅 방법을 적용한 결정 트리의 앙상블\n",
    "+ max_samples를 훈련 세트의 크기로 지정\n",
    "+ $\\sqrt{특성}$ 만큼의 특성 개수를 사용\n",
    "+ 트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 대신 무작위로 선택한 특성 후보 중에서 최적의 특성을 찾는 식\n",
    "+ 편향을 손해보는 대신 분산을 낮추어 더 훌륭한 모델을 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08e4e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1684ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(max_features=\"sqrt\", max_leaf_nodes=16),\n",
    "    n_estimators=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ac84f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2128bebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 거의 에측이 동일\n",
    "np.sum(y_pred == y_pred_rf) / len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca62591",
   "metadata": {},
   "source": [
    "### 7.4.1 엑스트라 트리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2434044",
   "metadata": {},
   "source": [
    "**익스트림 랜덤 트리(extremely randomized tree)** : 극단적으로 무작위한 트리의 랜덤 포레스트\n",
    "+ 최적의 임계값을 찾는 대신 후보 특성을 사용해 무작위로 분할한 다음 그중에서 최상의 분할을 선택\n",
    "+ 편향이 늘어나지만 대신 분산을 낮춤\n",
    "+ 일반적인 랜덤 포레스트보다 빠른 속도\n",
    "+ ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de317de4",
   "metadata": {},
   "source": [
    "### 7.4.2 특성 중요도\n",
    "**feature_importances_**\n",
    "+ 어떤 특성을 사용한 노드가 (랜덤 포레스트에 있는 모든 트리에 걸쳐서) 평균적으로 불순도를 얼마나 감소시키는지 확인하여 특성의 중요도를 측정\n",
    "+ 결정 트리를 기반으로 하는 모델은 모두 특성 중요도를 제공\n",
    "+ 중요도의 전체 합 = 1\n",
    "+ 어떤 특성이 중요한지 빠르게 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "081b048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.11249225099876375\n",
      "sepal width (cm) 0.02311928828251033\n",
      "petal length (cm) 0.4410304643639577\n",
      "petal width (cm) 0.4233579963547682\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26bc8244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11249225, 0.02311929, 0.44103046, 0.423358  ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fdb8d5",
   "metadata": {},
   "source": [
    "MNIST 데이터셋에 랜덤 포레스트 분류기를 훈련시키고 각 픽셀의 중요도를 그래프로 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f5f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.target = mnist.target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rnd_clf.fit(mnist[\"data\"], mnist[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d571c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = mpl.cm.hot,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e3b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digit(rnd_clf.feature_importances_)\n",
    "\n",
    "cbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(), rnd_clf.feature_importances_.max()])\n",
    "cbar.ax.set_yticklabels(['Not important', 'Very important'])\n",
    "\n",
    "save_fig(\"mnist_feature_importance_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f4fd7b",
   "metadata": {},
   "source": [
    "## 7.5 부스팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9077e7b",
   "metadata": {},
   "source": [
    "**부스팅(booasting)** : 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법\n",
    "+ 앞의 모델을 보완해나가면서 일련의 예측기를 학습\n",
    "+ 에이다부스트, 그레이디언트 부스팅\n",
    "\n",
    "### 7.5.1 에이다부스트(AdaBoost)\n",
    "**방식**\n",
    "\n",
    "이전 모델이 과소적합했던 훈련 샘플의 가중치를 높이면 새로운 예측기는 학습하기 어려운 샘플에 점점 더 맞춰지게 됨\n",
    "\n",
    "**과정**\n",
    "+ 첫 번째 분류기를 훈련 세트에서 훈련시키고 예측을 만듦\n",
    "+ 그다음에 알고리즘이 잘못 분류된 훈련 샘플의 가중치를 상대적으로 높임\n",
    "+ 업데이트된 가중치를 사용해 훈련 세트에서 훈련하고 다시 예측을 만듦\n",
    "+ 그다음에 다시 가중치를 업데이트함\n",
    "\n",
    "**특징**\n",
    "+ 에이다부스트는 점차 더 좋아지도록 앙상블에 예측기를 추가\n",
    "    + 경사 하강법(비용 함수를 최소화하기 위해 한 예측기의 모델 파라미터를 조정)과 비슷\n",
    "+ 모든 예측기가 훈련을 마치면 앙상블은 배깅이나 페이스팅과 비슷한 방식으로 예측을 만듦\n",
    "+ 사이킷런은 SAMME라는 에이다부스트의 다중 클래스 버전을 사용(예측기가 클래스의 확률을 추정할 수 있다면 SAMME.R 사용)\n",
    "\n",
    "<img src=\"img/7-7.png\" width=\"500px\" align='left'>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8234691e",
   "metadata": {},
   "source": [
    "moons 데이터셋에 훈련시킨 다섯 개의 연속된 예측기의 결정 경계\n",
    "+ 잘못 분류된 샘플의 가중치는 반복마다 절반 정도만 높아짐\n",
    "\n",
    "<img src=\"img/7-8.png\" width=\"500px\" align='left'>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd719e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoostClassifier을 사용해 200개의 아주 얕은 결정 트리를 기반으로 하는 에이다부스트 분류기를 훈련\n",
    "# max_depth = 1 (결정 노드 1개, 리프 노드 2개)\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad30ce87",
   "metadata": {},
   "source": [
    "### 7.5.2 그레이디언트 부스팅\n",
    "**방식**\n",
    "\n",
    "앙상블에 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가\n",
    "+ 반복마다 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 잔여 오차에 새로운 예측기를 학습\n",
    "+ 오차에 대해 학습하는 회귀 모델이기 때문에 회귀/분류 모델에 상관없이 DecisionTreeRegressor 고정\n",
    "\n",
    "**과정**\n",
    "+ DecisionTreeRegressor를 훈련 세트에 학습\n",
    "+ 첫 번째 예측기에서 생긴 잔여 오차에 두 번째 DecisionTreeRegressor를 훈련\n",
    "+ 두 번째 예측기가 만든 잔여 오차에 세 번째 회귀 모델을 훈련\n",
    "+ 새로운 샘플에 대한 예측을 만들기 위해 모든 트리의 예측을 더하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c3739",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c855d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce30892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025aa19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b92c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(regressors, X, y, axes, label=None, style=\"r-\", data_style=\"b.\", data_label=None):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500)\n",
    "    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n",
    "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
    "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
    "    if label or data_label:\n",
    "        plt.legend(loc=\"upper center\", fontsize=16)\n",
    "    plt.axis(axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,11))\n",
    "\n",
    "plt.subplot(321)\n",
    "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h_1(x_1)$\", style=\"g-\", data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.title(\"Residuals and tree predictions\", fontsize=16)\n",
    "\n",
    "plt.subplot(322)\n",
    "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.title(\"Ensemble predictions\", fontsize=16)\n",
    "\n",
    "plt.subplot(323)\n",
    "plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_2(x_1)$\", style=\"g-\", data_style=\"k+\", data_label=\"Residuals\")\n",
    "plt.ylabel(\"$y - h_1(x_1)$\", fontsize=16)\n",
    "\n",
    "plt.subplot(324)\n",
    "plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "\n",
    "plt.subplot(325)\n",
    "plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_3(x_1)$\", style=\"g-\", data_style=\"k+\")\n",
    "plt.ylabel(\"$y - h_1(x_1) - h_2(x_1)$\", fontsize=16)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "\n",
    "plt.subplot(326)\n",
    "plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "\n",
    "save_fig(\"gradient_boosting_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fbf8d5",
   "metadata": {},
   "source": [
    "**GradientBoostingRegressor**\n",
    "+ n_estimators, max_depth, min_samples_leaf\n",
    "+ learning_rate : 각 트리의 기여 정도를 조절\n",
    "    + **축소(shrinkage)**\n",
    "        + 학습률 낮추면 앙상블을 훈련 세트에 학습시키기 위해 많은 트리가 필요하지만 일반적으로 예측의 성능은 좋아짐\n",
    "+ loss : 최소제곱을 의미하는 'ls'가 기본값\n",
    "\n",
    "\n",
    "**GradientBoostingClassifier**\n",
    "+ loss : 로지스틱 손실 함수를 의미하는 'deviance'가 기본값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3138fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f420c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_slow = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=0.1, random_state=42)\n",
    "gbrt_slow.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cf585e",
   "metadata": {},
   "source": [
    "LEFT : 훈련 세트를 학습하기에는 트리가 충분하지 않음\n",
    "\n",
    "RIGHT : 트리가 너무 많아 훈련 세트에 과대적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4930f14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"Ensemble predictions\")\n",
    "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt.learning_rate, gbrt.n_estimators), fontsize=14)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_predictions([gbrt_slow], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
    "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt_slow.learning_rate, gbrt_slow.n_estimators), fontsize=14)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "\n",
    "save_fig(\"gbrt_learning_rate_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9ff782",
   "metadata": {},
   "source": [
    "**조기 종료 기법**\n",
    "+ staged_predict()\n",
    "    + 훈련의 각 단계에서 앙상블에 의해 만들어진 예측기를 순회하는 반복자를 반환\n",
    "+ 최적의 트리 수를 찾기 위해 각 훈련 단계에서 검증 오차를 측정\n",
    "+ 최적의 트리 수를 사용해 새로운 GBRT 앙상블을 훈련\n",
    "+ warm_start = True : 사이킷런이 fit() 메서드가 호출될 떄 기존 트리를 유지하고 훈련을 추가할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f2be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
    "\n",
    "bst_n_estimators = np.argmin(errors) + 1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27860be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_error = np.min(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e52325",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(np.arange(1, len(errors) + 1), errors, \"b.-\")\n",
    "plt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], \"k--\")\n",
    "plt.plot([0, 120], [min_error, min_error], \"k--\")\n",
    "plt.plot(bst_n_estimators, min_error, \"ko\")\n",
    "plt.text(bst_n_estimators, min_error*1.2, \"Minimum\", ha=\"center\", fontsize=14)\n",
    "plt.axis([0, 120, 0, 0.01])\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Error\", fontsize=16)\n",
    "plt.title(\"Validation error\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
    "plt.title(\"Best model (%d trees)\" % bst_n_estimators, fontsize=14)\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "\n",
    "save_fig(\"early_stopping_gbrt_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd18522",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break  # early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e446e21",
   "metadata": {},
   "source": [
    "**확률적 그레이디언트 부스팅(stochastic gradient boosting)**\n",
    "+ subsample : 각 트리가 훈련할 때 사용할 훈련 샘플의 비율\n",
    "    + 편향이 높아지는 대신 분산이 낮아짐\n",
    "    + 훈련 속도 증가\n",
    "\n",
    "**XGBoost(extreme gradient boosting)** : 최적화된 그레이디언트 부스팅 구현\n",
    "+ 매우 빠른 속도, 확장성, 이식성\n",
    "\n",
    "\n",
    "**LightGBM** : 히스토그램 기반 그레이디언트 부스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bd5a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg.fit(X_train,y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752300fd",
   "metadata": {},
   "source": [
    "## 7.6 스태킹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf88f75",
   "metadata": {},
   "source": [
    "**스태킹(stacking)** : 앙상블에 속한 모든 예측기의 예측을 취합하는 간단한 함수를 사용하는 대신 취합하는 모델을 훈련\n",
    "+ 각각 다른 값을 예측하고 마지막 예측기(블렌더 or 메타 학습기)가 이 예측을 입력으로 받아 최종 예측을 만듦\n",
    "<img src=\"img/7-12.png\" width=\"500px\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78cb4a6",
   "metadata": {},
   "source": [
    "**블렌더(blender) 학습 방법**\n",
    "+ 홀드 아웃(hold-out) 세트를 사용\n",
    "1. 훈련 세트를 두 개의 서브셋으로 나눔\n",
    "2. 첫 번째 세트는 첫 번째 레이어의 예측을 훈련시키기 위해 사용\n",
    "3. 첫 번째 레이어의 예측기를 사용해 두 번째 세트에 대한 예측을 만듦\n",
    "+ 첫 번째 레이어의 예측을 가지고 타깃값을 예측\n",
    "    \n",
    "<img src=\"img/7-13.png\" width=\"500px\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbe60b1",
   "metadata": {},
   "source": [
    "+ 블렌더를 여러 개 훈련\n",
    "\n",
    "<img src=\"img/7-14.png\" width=\"500px\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f70636",
   "metadata": {},
   "source": [
    "+ 각 레이어를 차례대로 실행해서 새로운 샘플에 대한 예측\n",
    "\n",
    "<img src=\"img/7-15.png\" width=\"500px\" align='left'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
